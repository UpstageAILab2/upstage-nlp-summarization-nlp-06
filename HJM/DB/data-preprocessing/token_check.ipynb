{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # 모델의 성능을 평가하기 위한 라이브러리입니다.\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration, BartConfig\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, T5Config\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb # 모델 학습 과정을 손쉽게 Tracking하고, 시각화할 수 있는 라이브러리입니다.\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/NanumFont/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n",
    "model_dir = \"lcw99/t5-base-korean-text-summary\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_dir) \n",
    "\n",
    "# https://huggingface.co/paust/pko-t5-base\n",
    "# transformers 의 API 를 사용하여 접근 가능합니다. \n",
    "# tokenizer 를 사용할때는 T5Tokenizer 가 아니라 T5TokenizerFast 를 사용해주십시오. \n",
    "# model 은 T5ForConditionalGeneration 를 그대로 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../../data\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.\n",
    "        \"model_name\": f\"{model_dir}\", # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.\n",
    "        \"output_dir\": \"./\" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1000,\n",
    "        \"decoder_max_len\": 175,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.\n",
    "        \"special_tokens\": ['#Person3#', '#PhoneNumber#', '#Person5#', '#PassportNumber#', '#DateOfBirth#', '#Address#', '#Person6#', \n",
    "                           '#CarNumber#', '#SSN#', '#Person7#', '#CardNumber#', '#Person4#', '#Person2#', '#Person#', '#Email#', '#Person1#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 50,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 50, # 50\n",
    "        \"per_device_eval_batch_size\": 32, # 32\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 3, # 1\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 7,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 175,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 5, # 3\n",
    "        \"early_stopping_threshold\": 0, # 0.001\n",
    "        \"report_to\": \"wandb\" # (선택) wandb를 사용할 때 설정합니다.\n",
    "    },\n",
    "    # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"entity_name\",\n",
    "        \"project\": \"project_name\",\n",
    "        \"name\": \"name\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\", # 사전 학습이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 175,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\" : 32, # 32\n",
    "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 config 파일을 불러옵니다.\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험에서 쓰일 데이터를 load하여 데이터의 구조와 내용을 살펴보겠습니다.\n",
    "\n",
    "Train, dev, test 순서대로 12457, 499, 250개 씩 데이터가 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n",
    "data_path = loaded_config['general']['data_path']\n",
    "\n",
    "# train data의 구조와 내용을 확인합니다.\n",
    "train_df = pd.read_csv(os.path.join(data_path,'train_data-preprocessing.csv'))\n",
    "display(train_df.tail())\n",
    "\n",
    "# dev data의 구조와 내용을 확인합니다.\n",
    "dev_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
    "display(dev_df.tail())\n",
    "\n",
    "# test data의 구조와 내용을 확인합니다.\n",
    "test_df = pd.read_csv(os.path.join(data_path,'test.csv'))\n",
    "display(test_df.tail())\n",
    "\n",
    "# train, dev, test 길이 확인\n",
    "print(train_df.shape[0], dev_df.shape[0], test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "special_tokens = ['#Person3#',\n",
    "                  '#PhoneNumber#',\n",
    "                  '#Person5#',\n",
    "                  '#PassportNumber#',\n",
    "                  '#DateOfBirth#',\n",
    "                  '#Address#',\n",
    "                  '#Person6#',\n",
    "                  '#CarNumber#',\n",
    "                  '#SSN#',\n",
    "                  '#Person7#',\n",
    "                  '#CardNumber#',\n",
    "                  '#Person4#',\n",
    "                  '#Person2#',\n",
    "                  '#Person#',\n",
    "                  '#Email#',\n",
    "                  '#Person1#']\n",
    "\n",
    "special_tokens_dict={'additional_special_tokens':special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "dialogue_tokenizer = train_df['dialogue'].apply(lambda x: tokenizer(x))\n",
    "summary_tokenizer = train_df['summary'].apply(lambda x: tokenizer(x))\n",
    "topic_tokenizer = train_df['topic'].apply(lambda x: tokenizer(x))\n",
    "\n",
    "dialogue_len = dialogue_tokenizer.apply(lambda x: len(x['input_ids']))\n",
    "summary_len = summary_tokenizer.apply(lambda x: len(x['input_ids']))\n",
    "topic_len = topic_tokenizer.apply(lambda x: len(x['input_ids']))\n",
    "\n",
    "temp = dialogue_len.describe().reset_index()\n",
    "temp0 = summary_len.describe().reset_index()\n",
    "temp1 = topic_len.describe().reset_index()\n",
    "\n",
    "temp.merge(temp0, on='index').merge(temp1, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot으로 분포 확인\n",
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(data=dialogue_len.reset_index(), x=dialogue_len.index, y='dialogue')\n",
    "plt.title('대화 길이')\n",
    "plt.xlabel('Index')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=summary_len.reset_index(), x=summary_len.index, y='summary')\n",
    "plt.title('요약문 길이')\n",
    "plt.xlabel('Index')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=topic_len.reset_index(), x=topic_len.index, y='topic')\n",
    "plt.title('토픽 길이')\n",
    "plt.xlabel('Index')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
